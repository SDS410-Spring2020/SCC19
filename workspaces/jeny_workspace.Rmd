---
title: "jeny_workspace"
author: "Jeny Kwon"
date: "4/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
library(rvest)
library(pdftools)
library(dplyr)
library(glue)
library(httr)
```

```{r}
# Scrape urls from main webpage with all the letters
url <- read_html("https://www.smith.edu/president-kathleen-mccartney/letters-community")

links <- url %>% 
  html_nodes(".field-item") %>%   
  html_nodes("li") %>% 
  html_nodes("a") %>%
  sub(pattern = '<a href="', replacement = "") %>%
  sub(pattern = '(\\".*)', replacement = "")
```


```{r}
# Remove main domain to have clean data- most are missing the domain url 
links_clean <- links %>% 
  str_replace("https://www.smith.edu", "") 
  
```

```{r}
# Add main domain page to each url
links_url <- paste("https://www.smith.edu", links_clean,  sep="")

```

```{r}
# # Testing purposes (less data)
# years = c("2019-20")
# Only include urls from 2017-18, 2018-19, and 2019-20
years = c("2019-20", "2018-19", "2017-18")

links_years <- links_url[!links_url %in% grep(paste0(years, collapse = "|"), links_url, value = T, invert = T)]

```


```{r}
# Check if each of the urls exists
urls_exist <- c()
check_exists <- function(links_years, urls_exist){
  if(GET(links_years, body = F)[2]==200){
    urls_exist <- c(urls_exist, links_years)
  }else{
    return()
  }
}

```


```{r}
# Only include urls that are valid/exist
working_urls <- lapply(links_years, check_exists, urls_exist)
links_actual <- unlist(working_urls)

```

```{r}
# Scrape title of each letter
title <- lapply(links_actual,
              function(links_actual) {
                links_actual %>% read_html() %>%
                html_node(".page-title") %>%
                html_text() %>% 
                sub(pattern = "\n", replacement = "")
              }
) 


```



```{r}
# Scrape content of each letter
content <- lapply(links_actual,
              function(links_actual) {
                links_actual %>% read_html() %>%
                html_node(".left-column-text") %>%
                html_node(".field-item.even") %>%
                html_text() %>% 
                sub(pattern = "\n", replacement = "")
              }
)


```


```{r}
# Get titles of each letter from the main webpage
titles_dates <- url %>%
  html_nodes(".field-item") %>%
  html_nodes("li") %>%
  html_nodes("a") %>%
  html_text()

# Unlist vector of 
title_dates <- unlist(titles_dates)

# Date format to search for
date_format <- "[A-Z]{1}[a-z]{1,10} [0-9]{1,2}, [0-9]{4}"

# Extract date using above format from titles
date <- lapply(title_dates,
                function(title_dates) {
                 str_extract(string = title_dates,
                             pattern = date_format)
                }
)

# Unlist dates vector
date <- unlist(date, use.names=FALSE)

# Unlink links 
# links_url from line 41
link <- unlist(links_url, use.names=FALSE)

# Create data frame with extracted dates and links
df_dates <- as.data.frame(cbind(date, link))

# df_dates
```



```{r}
# Unlist extracted text from each category
title <- unlist(title, use.names=FALSE) 

content <- unlist(content, use.names=FALSE)

link <- unlist(links_actual, use.names=FALSE)

# Combine data into a data frame with titles, contents, and links
df_content <- as.data.frame(cbind(title, content, link))

df_content

```


```{r}
# Join data data frame with dates with data frame with content
df_full <- inner_join(df_content, df_dates,  by = "link")

# Change data types
df_full$link <- 
                 as.character(as.factor(df_full$link))

df_full$title <-
                as.character(as.factor(df_full$title))

df_full$content <-
                as.character(as.factor(df_full$content))

df_full$date <-
                as.Date(as.factor(df_full$date), format ="%B %d, %Y")

df_full <- tibble::rowid_to_column(df_full, "ID")


# df_full
```

```{r}
# Trasnform to tidy text data and remove words such as "of, the, to, in"
library(tidytext)

df_tidy <- df_full %>% 
  unnest_tokens(word, content) %>% 
  anti_join(stop_words)

df_tidy <- df_tidy %>% 
  group_by(ID) %>% 
  mutate(wordcount = row_number()) %>% 
  ungroup()
```

```{r}
# Most used words in the letters
df_tidy %>% 
  count(word, sort = TRUE)
```

```{r}
# Most common words in all the letters
df_tidy %>% 
  count(word, sort = TRUE) %>% 
  filter (n > 50) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip()
```

```{r}
library(dplyr)
library(stringr)
library(textdata)

nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

nrc_sadness <- get_sentiments("nrc") %>% 
  filter(sentiment == "sadness")
```

```{r}
# The 142nd Commencement of Smith College: A Virtual Celebration, April 14, 2020
joy_1 <- df_tidy %>% 
  filter(ID == "1") %>% 
  inner_join(nrc_joy) %>% 
  count(word, sort = TRUE)

joy_1
```


```{r}
# The 142nd Commencement of Smith College: A Virtual Celebration, April 14, 2020
sadness_1 <- df_tidy %>% 
  filter(ID == "1") %>% 
  inner_join(nrc_sadness) %>% 
  count(word, sort = TRUE)

sadness_1

```

```{r}
# A Message from President Kathleen McCartney Regarding Commencement and Reunions, March 18, 2020
joy_1 <- df_tidy %>% 
  filter(ID == "9") %>% 
  inner_join(nrc_joy) %>% 
  count(word, sort = TRUE)

joy_1
```

```{r}
# A Message from President Kathleen McCartney Regarding Commencement and Reunions, March 18, 2020
sadness_1 <- df_tidy %>% 
  filter(ID == "9") %>% 
  inner_join(nrc_sadness) %>% 
  count(word, sort = TRUE)

sadness_1

```


```{r}
library(tidyr)
library(ggplot2)

select <- c(1, 2, 3, 4)

first_four_sentiment <- df_tidy %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(ID, index = wordcount, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  filter(ID %in% select)

ggplot(first_four_sentiment, aes(index, sentiment, fill = ID)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ID, ncol = 2, scales = "free_x")
```


