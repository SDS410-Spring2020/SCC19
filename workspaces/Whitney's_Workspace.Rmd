---
title: "Whitney's Workspace"
author: "Whitney Mutalemwa"
date: "4/13/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#loading libraries
library(tidytext)
library(stringr)
library(tidyverse)
library(wordcloud)
library(dplyr)
library(reshape2)
library(lubridate)
library(ggplot2)
```

```{r}
#load the rda file
load(file = "textdata.Rda")

#code to get started
#"pasted this part from Emily :), original code by Jeny :)"
df_tidy <- df_full %>% 
  unnest_tokens(word, content) %>% 
  anti_join(stop_words) 

#simple frequency code for common words
#df_count <- df_tidy %>% 
  #count(word, sort = TRUE)

#word cloud of 100 most frequently written words
df_word <- df_tidy %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

#calls for bing sentiment lexicon 
bing_lex = get_sentiments("bing")

#categorizing words as positive or negative
df_bing <- df_tidy %>%
  right_join(bing_lex) %>%
  filter(!is.na(sentiment)) %>%
  count(sentiment, sort = TRUE)

#word cloud of 50 most frequently used negative and positive words
df_bingword <- df_tidy %>%
  inner_join(bing_lex) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "green"),
                   max.words = 50)
```

```{r}
#each year splits into groups of 50 words (we can vary this)
#sentiment score is calculated
bing_score <- df_tidy %>%
  mutate(year = year(date)) %>%
  group_by(year) %>% 
  mutate(word_count = 1:n(),
         index = word_count %/% 50 + 1) %>% 
  inner_join(bing_lex, by = "word") %>%
  count(year, index = index , sentiment) %>%
  ungroup() %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative,
         date = factor(year)) %>%
  ggplot(aes(index, sentiment, fill = year)) +
  geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ date, ncol = 2, scales = "free_x")
bing_score
```

```{r}

#future coding: look into pair of words (bigrams)

#separates text into pairs, eliminates stop words
df_pairs <- df_full %>%
  unnest_tokens(bigram, content, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ")%>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
#df_pairs

#simple frequency code for pair of words
#df_counts <- df_pairs %>% 
  #count(word1, word2, sort = TRUE)

#joins the two separate stings into one string
df_united <- df_pairs %>%
  unite(bigram, word1, word2, sep = " ")
#df_united %>%
  #count(bigram, sort = TRUE)
```

```{r}
#future coding: look into pair of words (bigrams)

#uses the term frequency -inverse document frequency aka tf-idf
#tf-idf is how frequent a pair word appears in one community letter
#given how often it appears in other community letters by YEAR
df_tf_idf <- df_united %>%
  mutate(year = year(date)) %>%
  group_by(year) %>%
  count(date, bigram) %>%
  bind_tf_idf(bigram, date, n) %>%
  arrange(desc(tf_idf))

#plot of the top ten bigrams with the highest tf-idf scores among
#among the overall community letters by YEAR
scc_plot <- df_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram))))

scc_plot %>% 
  top_n(15) %>%
  ungroup() %>%
  ggplot(aes(bigram, tf_idf, fill = factor(year))) +
  geom_col() +
  facet_wrap(~factor(year), ncol = 2, scales = "free") +
  labs(x = NULL, y = "tf-idf") +
  coord_flip()
```