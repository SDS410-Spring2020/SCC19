---
title: "url_scraping"
author: "Jeny Kwon"
date: "4/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(pdftools)
library(dplyr)
library(glue)
library(httr)
```


```{r}
# Scrape urls from main webpage with all the letters
url <- read_html("https://www.smith.edu/president-kathleen-mccartney/letters-community")

links <- url %>% 
  html_nodes(".field-item") %>%   
  html_nodes("li") %>% 
  html_nodes("a") %>%
  sub(pattern = '<a href="', replacement = "") %>%
  sub(pattern = '(\\".*)', replacement = "")
```



```{r}
# Remove main domain to have clean data- most are missing the domain url 
links_clean <- links %>% 
  str_replace("https://www.smith.edu", "") 
  
```

```{r}
# Only include urls from 2017-18, 2018-19, and 2019-20
years = c("2019-20", "2018-19", "2017-18")

links_years <- links_clean[!links_clean %in% grep(paste0(years, collapse = "|"), links_clean, value = T, invert = T)]

```

```{r}
# Add main domain page to each url
links_url <- paste("https://www.smith.edu", links_years,  sep="")

```

```{r}
# Check if each of the urls exists
urls_exist <- c()
check_exists <- function(links_url, urls_exist){
  if(GET(links_url, body = F)[2]==200){
    urls_exist <- c(urls_exist, links_url)
  }else{
    return()
  }
}

working_urls <- lapply(links_url, check_exists, urls_exist)


```


```{r}
# Only include urls that are valid/exist
links_actual <- unlist(working_urls)

```

```{r}
# Scrape title of each letter
title <- lapply(links_actual,
              function(links_actual) {
                links_actual %>% read_html() %>%
                html_node(".page-title") %>%
                html_text() %>% 
                sub(pattern = "\n", replacement = "")
              }
) 

# Scrape content of each letter
content <- lapply(links_actual,
              function(links_actual) {
                links_actual %>% read_html() %>%
                html_node(".left-column-text") %>%
                html_node(".field-item.even") %>%
                html_text() %>% 
                sub(pattern = "\n", replacement = "")
              }
)

# Get titles of each letter from the main webpage
titles_dates <- url %>%
  html_nodes(".field-item") %>%
  html_nodes("li") %>%
  html_nodes("a") %>%
  html_text()

# Unlist vector of 
title_dates <- unlist(titles_dates)

# Date format to search for
date_format <- "[A-Z]{1}[a-z]{1,10} [0-9]{1,2}, [0-9]{4}"

# Extract date using above format from titles
date <- lapply(title_dates,
                function(title_dates) {
                 str_extract(string = title_dates,
                             pattern = date_format)
                }
)

# Unlist dates vector
date <- unlist(date, use.names=FALSE)

# Unlink links 
# links_url from line 41
link <- unlist(links_url, use.names=FALSE)

# Create data frame with extracted dates and links
df_dates <- as.data.frame(cbind(date, link))
```



```{r}
# Unlist extracted text from each category
title <- unlist(title, use.names=FALSE) 

content <- unlist(content, use.names=FALSE)

link <- unlist(links_actual, use.names=FALSE)

# Combine data into a data frame with titles, contents, and links
df_content <- as.data.frame(cbind(title, content, link))

# Join data data frame with dates with data frame with content
df_full <- inner_join(df_content, df_dates,  by = "link")

# Change data types
df_full$link <- 
                 as.character(as.factor(df_full$link))

df_full$title <-
                as.character(as.factor(df_full$title))

df_full$content <-
                as.character(as.factor(df_full$content))

df_full$date <-
                as.Date(as.factor(df_full$date), format ="%B %d, %Y")

df_full <- tibble::rowid_to_column(df_full, "ID")

write.csv(df_full, "df_full.csv")

```


```{r}
# Trasnform to tidy text data and remove words such as "of, the, to, in"
library(tidytext)

df_tidy <- df_full %>% 
  unnest_tokens(word, content) %>% 
  anti_join(stop_words)

df_tidy <- df_tidy %>% 
  group_by(ID) %>% 
  mutate(wordcount = row_number()) %>% 
  ungroup()

write.csv(df_tidy, "df_tid.csv" )
```

