---
title: "jeny_workspace"
author: "Jeny Kwon"
date: "4/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
library(rvest)
library(pdftools)
library(dplyr)
library(glue)
#library(RCurl) 
library(httr)
library(formattable)
```

```{r}
# Scrape urls from main webpage with all the letters
url <- read_html("https://www.smith.edu/president-kathleen-mccartney/letters-community")

topics <- url %>% 
  html_nodes(".field-item") %>%   
  html_nodes("li") %>% 
  html_nodes("a") %>%
  html_text()

links <- url %>% 
  html_nodes(".field-item") %>%   
  html_nodes("li") %>% 
  html_nodes("a") %>%
  sub(pattern = '<a href="', replacement = "") %>%
  sub(pattern = '(\\".*)', replacement = "")
```

```{r}
# Remove main domain to have clean data- most are missing the domain url 
links_clean <- links %>% 
  str_replace("https://www.smith.edu", "") 
  
```

```{r}
# Only include urls from 2017-18, 2018-19, and 2019-20
years = c("2019-20", "2018-19", "2017-18")

links_years <- links_clean[!links_clean %in% grep(paste0(years, collapse = "|"), links_clean, value = T, invert = T)]

```

```{r}
# # Testing purposes (less data)
# years = c("2019-20")
# 
# links_years <- links_clean[!links_clean %in% grep(paste0(years, collapse = "|"), links_clean, value = T, invert = T)]

```


```{r}
# Add main domain page to each url
links_url <- paste("https://www.smith.edu", links_years,  sep="")

```

```{r}
# Check if each of the urls exists
urls_exist <- c()
check_exists <- function(links_url, urls_exist){
  if(GET(links_url, body = F)[2]==200){
    urls_exist <- c(urls_exist, links_url)
  }else{
    return()
  }
}

working_urls <- lapply(links_url, check_exists, urls_exist)

```


```{r}
# Only include urls that are valid/exist
links_actual <- unlist(working_urls)
```


```{r}
# url <- "https://www.smith.edu/president-kathleen-mccartney/letters/2019-20/april-10-2020"
# 
# 
# title <- read_html(url) %>% 
#   html_node(".page-title") %>% 
#   html_text()
# 
# title
```

```{r}
# url <- "https://www.smith.edu/president-kathleen-mccartney/letters/2019-20/april-10-2020"
# 
# 
# text <- read_html(url) %>% 
#   html_node(".left-column-text") %>% 
#   html_node(".field-item.even") %>% 
#   html_text()
# 
# text
```


```{r}
# Scrape title of each letter
title <- lapply(links_actual,
              function(links_actual) {
                links_actual %>% read_html() %>%
                html_node(".page-title") %>%
                html_text()
              }
)

# Scrape academic year each letter was posted
year <- lapply(links_actual,
              function(links_actual) {
                links_actual %>% read_html() %>%
                html_node(".breadcrumbcurrent") %>% 
                html_node("a") %>% 
                html_text()

              }
)

# Scrape content of each letter
content <- lapply(links_actual,
              function(links_actual) {
                links_actual %>% read_html() %>%
                html_node(".left-column-text") %>%
                html_node(".field-item.even") %>%
                html_text()
              }
)



```

```{r}
# get date of publication from urls
datepattern <- "[a-z]{1,10}-[0-9]{2}-[0-9]{4}"
date <- lapply(links_actual,
               function(links_actual) {
                 datelocate <- regexpr(datepattern, links_actual)[1]
                 return(substr(links_actual, datelocate, nchar(links_actual)))
                   })
url <- "https://www.smith.edu/president-kathleen-mccartney/letters/2019-20/april-10-2020"
datelocate <- regexpr(datepattern, url)[1]
substr(url, datelocate, nchar(url))

```

```{r}
# Unlist extracted text from each category
title <- unlist(title, use.names=FALSE)

year <- unlist(year, use.names=FALSE)

content <- unlist(content, use.names=FALSE)

link <- unlist(links_actual, use.names=FALSE)

# Combine data into a data frame
df <- as.data.frame(cbind(title, year, content, link))

formattable(df)


```









