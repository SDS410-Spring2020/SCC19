---
title: "jeny_workspace"
author: "Jeny Kwon"
date: "4/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
library(rvest)
library(pdftools)
library(dplyr)
library(glue)
#library(RCurl) 
library(httr)
library(formattable)
```

```{r}
# Scrape urls from main webpage with all the letters
url <- read_html("https://www.smith.edu/president-kathleen-mccartney/letters-community")

links <- url %>% 
  html_nodes(".field-item") %>%   
  html_nodes("li") %>% 
  html_nodes("a") %>%
  sub(pattern = '<a href="', replacement = "") %>%
  sub(pattern = '(\\".*)', replacement = "")
```

```{r}
# Remove main domain to have clean data- most are missing the domain url 
links_clean <- links %>% 
  str_replace("https://www.smith.edu", "") 
  
```

```{r}
# Only include urls from 2017-18, 2018-19, and 2019-20
years = c("2019-20", "2018-19", "2017-18")

links_years <- links_clean[!links_clean %in% grep(paste0(years, collapse = "|"), links_clean, value = T, invert = T)]

```

```{r}
# # Testing purposes (less data)
# years = c("2019-20")
# 
# links_years <- links_clean[!links_clean %in% grep(paste0(years, collapse = "|"), links_clean, value = T, invert = T)]

```


```{r}
# Add main domain page to each url
links_url <- paste("https://www.smith.edu", links_years,  sep="")

```

```{r}
# Check if each of the urls exists
urls_exist <- c()
check_exists <- function(links_url, urls_exist){
  if(GET(links_url, body = F)[2]==200){
    urls_exist <- c(urls_exist, links_url)
  }else{
    return()
  }
}

working_urls <- lapply(links_url, check_exists, urls_exist)

```


```{r}
# Only include urls that are valid/exist
links_actual <- unlist(working_urls)
```


```{r}
# url <- "https://www.smith.edu/president-kathleen-mccartney/letters/2019-20/april-10-2020"
# 
# 
# title <- read_html(url) %>% 
#   html_node(".page-title") %>% 
#   html_text()
# 
# title
```

```{r}
# url <- "https://www.smith.edu/president-kathleen-mccartney/letters/2019-20/april-10-2020"
# 
# 
# text <- read_html(url) %>% 
#   html_node(".left-column-text") %>% 
#   html_node(".field-item.even") %>% 
#   html_text()
# 
# text
```


```{r}
# Scrape title of each letter
title <- lapply(links_actual,
              function(links_actual) {
                links_actual %>% read_html() %>%
                html_node(".page-title") %>%
                html_text()
              }
)

# Scrape academic year each letter was posted
year <- lapply(links_actual,
              function(links_actual) {
                links_actual %>% read_html() %>%
                html_node(".breadcrumbcurrent") %>% 
                html_node("a") %>% 
                html_text()

              }
)

# Scrape content of each letter
content <- lapply(links_actual,
              function(links_actual) {
                links_actual %>% read_html() %>%
                html_node(".left-column-text") %>%
                html_node(".field-item.even") %>%
                html_text()
              }
)




```

```{r}
# get date of publication from titles
# this works for everything not from the last ~2 months
dateformat <- "[A-Z]{1}[a-z]{1,10} [0-9]{1,2}, [0-9]{4}"
older_dates <- lapply(links_actual,
               function(links_actual) {
                str_extract(string = (links_actual %>%
                                        read_html() %>%
                                        html_nodes(".field-item.even") %>%
                                        html_text())[1],
                            pattern = dateformat)
               }
)

# this works for the most recent links
datepattern <- "[a-z]{1,10}-[0-9]{1,2}-[0-9]{4}"
new_dates <- lapply(links_actual,
                    function(links_actual) {
                      datelocate <- regexpr(datepattern, links_actual)[1]
                      return(substr(links_actual, datelocate, nchar(links_actual)))
                    }
)

# note: links_actual[11] has no date associated with it in either of these formats. I think it was published on Feb.27 2020. We may have to hard code this in. 

```

```{r}
# Unlist extracted text from each category
title <- unlist(title, use.names=FALSE)

year <- unlist(year, use.names=FALSE)

content <- unlist(content, use.names=FALSE)

link <- unlist(links_actual, use.names=FALSE)

# Combine data into a data frame
df <- as.data.frame(cbind(title, year, content, link))

formattable(df)


```









