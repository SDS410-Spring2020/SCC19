---
title: "jeny_workspace"
author: "Jeny Kwon"
date: "4/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
library(rvest)
library(pdftools)
library(dplyr)
library(glue)
library(RCurl) 
library(formattable)
```

```{r}
# Scrape urls from main webpage with all the letters
url <- read_html("https://www.smith.edu/president-kathleen-mccartney/letters-community")

links <- url %>% 
  html_nodes(".field-item") %>%   
  html_nodes("li") %>% 
  html_nodes("a") %>% 
  html_text()

```

```{r}
# Remove main domain to have clean data- most are missing the domain url 
links_clean <- links %>% 
  str_replace("https://www.smith.edu", "") 
  
```

```{r}
# Only include urls from 2017-18, 2018-19, and 2019-20
years = c("2019-20", "2018-19", "2017-18")

links_years <- links_clean[!links_clean %in% grep(paste0(years, collapse = "|"), links_clean, value = T, invert = T)]

```

```{r}
# # Testing purposes (less data)
# years = c("2019-20")
# 
# links_years <- links_clean[!links_clean %in% grep(paste0(years, collapse = "|"), links_clean, value = T, invert = T)]

```


```{r}
# Add main domain page to each url
links_url <- paste("https://www.smith.edu", links_years,  sep="")

```

```{r}
# Check if each of the urls exists
links_exist <- url.exists(links_url)
```


```{r}
# Only include urls that are valid/exist
links_actual <- links_url[links_exist]
```


```{r}
# url <- "https://www.smith.edu/president-kathleen-mccartney/letters/2019-20/april-10-2020"
# 
# 
# title <- read_html(url) %>% 
#   html_node(".page-title") %>% 
#   html_text()
# 
# title
```

```{r}
# url <- "https://www.smith.edu/president-kathleen-mccartney/letters/2019-20/april-10-2020"
# 
# 
# text <- read_html(url) %>% 
#   html_node(".left-column-text") %>% 
#   html_node(".field-item.even") %>% 
#   html_text()
# 
# text
```


```{r}
# Scrape title of each letter
title <- lapply(links_actual,
              function(links_actual) {
                links_actual %>% read_html() %>%
                html_node(".page-title") %>%
                html_text()
              }
)

# Scrape academic year each letter was posted
year <- lapply(links_actual,
              function(links_actual) {
                links_actual %>% read_html() %>%
                html_node(".breadcrumbcurrent") %>% 
                html_node("a") %>% 
                html_text()

              }
)

# Scrape content of each letter
content <- lapply(links_actual,
              function(links_actual) {
                links_actual %>% read_html() %>%
                html_node(".left-column-text") %>%
                html_node(".field-item.even") %>%
                html_text()
              }
)
```

```{r}
# Unlist extracted text from each category
title <- unlist(title, use.names=FALSE)

year <- unlist(year, use.names=FALSE)

content <- unlist(content, use.names=FALSE)

link <- unlist(links_actual, use.names=FALSE)

# Combine data into a data frame
df <- as.data.frame(cbind(title, year, content, link))

formattable(df)


```









